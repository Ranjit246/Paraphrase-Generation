{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Install Dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install sentencepiece\n!pip install transformers\n!pip install pytorch_lightning\n!pip install -U numpy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-27T05:17:48.436116Z","iopub.execute_input":"2022-07-27T05:17:48.436653Z","iopub.status.idle":"2022-07-27T05:18:32.797725Z","shell.execute_reply.started":"2022-07-27T05:17:48.436549Z","shell.execute_reply":"2022-07-27T05:18:32.796341Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Import Dependencies**","metadata":{}},{"cell_type":"code","source":"import argparse  \nimport glob   \nimport os\nimport csv\nimport json\nimport time\nimport logging  \nimport random    \nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nimport pandas as pd\nimport numpy as np\n\n\nfrom transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:32.800214Z","iopub.execute_input":"2022-07-27T05:18:32.800619Z","iopub.status.idle":"2022-07-27T05:18:42.710967Z","shell.execute_reply.started":"2022-07-27T05:18:32.800580Z","shell.execute_reply":"2022-07-27T05:18:42.709870Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Set_Seed Defined**","metadata":{}},{"cell_type":"code","source":"def set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:42.712516Z","iopub.execute_input":"2022-07-27T05:18:42.713252Z","iopub.status.idle":"2022-07-27T05:18:42.784007Z","shell.execute_reply.started":"2022-07-27T05:18:42.713208Z","shell.execute_reply":"2022-07-27T05:18:42.783051Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Fine Tuner Class**","metadata":{}},{"cell_type":"code","source":"class T5FineTuner(pl.LightningModule):\n\n  def __init__(self,hparams):\n\n    # Calling the super constructer\n    super().__init__()\n    self.params = hparams\n\n    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n\n\n  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None):\n\n    return self.model(input_ids, attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,)\n    \n  def is_logger(self):\n      return self.trainer.global_rank <= 0\n    \n\n  def _step(self, batch):\n        labels = batch[\"target_ids\"]\n        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            labels=labels,\n            decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n\n        return loss\n\n  def training_step(self, batch, batch_idx):\n      loss = self._step(batch)\n\n      tensorboard_logs = {\"train_loss\": loss}\n      return {\"loss\": loss, \"log\": tensorboard_logs}\n\n#   def training_epoch_end(self, outputs):\n#       avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n#       tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n#       return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def validation_step(self, batch, batch_idx):\n      loss = self._step(batch)\n      return {\"val_loss\": loss}\n\n  def validation_epoch_end(self, outputs):\n      avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n      tensorboard_logs = {\"val_loss\": avg_loss}\n      return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n\n  def configure_optimizers(self):\n    \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n    model = self.model\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.params.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.1,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=self.params.learning_rate, eps=self.params.adam_epsilon)\n    self.opt = optimizer\n    return [optimizer]\n\n\n  # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=None, on_tpu=None, using_lbfgs=None, optimizer_closure=None):\n  #   optimizer.step(closure=optimizer_closure)\n  #   optimizer.zero_grad()\n  #   self.lr_scheduler.step()\n\n\n  def get_tqdm_dict(self):\n    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n    return tqdm_dict\n\n  def train_dataloader(self):\n    train_dataset = CustomDataset(tokenizer=self.tokenizer, type_path=\"../input/paw-paraphrase-generation/PAW_Train\",data_dir=self.params.data_dir, max_len=self.params.max_seq_length)\n    dataloader = DataLoader(train_dataset, batch_size=self.params.train_batch_size, drop_last=True, shuffle=True,\n                            num_workers=4)\n    t_total = (\n            (len(dataloader.dataset) // (self.params.train_batch_size * max(1, self.params.n_gpu)))\n            // self.params.gradient_accumulation_steps\n            * float(self.params.num_train_epochs)\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        self.opt, num_warmup_steps=self.params.warmup_steps, num_training_steps=t_total\n    )\n    self.lr_scheduler = scheduler\n    return dataloader\n\n  def val_dataloader(self):\n    val_dataset = CustomDataset(tokenizer=self.tokenizer, type_path=\"../input/paw-paraphrase-generation/PAW_Test\",data_dir=self.params.data_dir, max_len=self.params.max_seq_length)\n    return DataLoader(val_dataset, batch_size=self.params.eval_batch_size, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:42.786959Z","iopub.execute_input":"2022-07-27T05:18:42.787373Z","iopub.status.idle":"2022-07-27T05:18:42.809846Z","shell.execute_reply.started":"2022-07-27T05:18:42.787332Z","shell.execute_reply":"2022-07-27T05:18:42.808700Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Logger**","metadata":{}},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n  def on_validation_end(self, trainer, pl_module):\n    logger.info(\"***** Validation results *****\")\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n      # Log results\n      for key in sorted(metrics):\n        if key not in [\"log\", \"progress_bar\"]:\n          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n  def on_test_end(self, trainer, pl_module):\n    logger.info(\"***** Test results *****\")\n\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n\n      # Log and save results to file\n      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n      with open(output_test_results_file, \"w\") as writer:\n        for key in sorted(metrics):\n          if key not in [\"log\", \"progress_bar\"]:\n            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:42.811256Z","iopub.execute_input":"2022-07-27T05:18:42.812183Z","iopub.status.idle":"2022-07-27T05:18:42.823373Z","shell.execute_reply.started":"2022-07-27T05:18:42.812143Z","shell.execute_reply":"2022-07-27T05:18:42.822455Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Hyper Parameters**","metadata":{}},{"cell_type":"code","source":"# Hyper parameters\nargs_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='t5-base',\n    tokenizer_name_or_path='t5-base',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.1,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=6,\n    eval_batch_size=6,\n    num_train_epochs=2,\n    gradient_accumulation_steps=16,\n    n_gpu=1,\n    # early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O2', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:42.826082Z","iopub.execute_input":"2022-07-27T05:18:42.826532Z","iopub.status.idle":"2022-07-27T05:18:42.838960Z","shell.execute_reply.started":"2022-07-27T05:18:42.826506Z","shell.execute_reply":"2022-07-27T05:18:42.837755Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Custom Dataset**","metadata":{}},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('t5-base')\n\nclass CustomDataset(Dataset):\n    def __init__(self, tokenizer, data_dir, type_path, max_len=256):\n        # self.path = os.path.join(data_dir, type_path + '.csv')\n\n        self.source_column = \"question1\"\n        self.target_column = \"question2\"\n        \n        self.data = []\n        \n        with open(type_path+\".csv\",\"r\") as csv_file:\n          csv_reader = csv.reader(csv_file, delimiter=',')\n          line_count = 0\n          for row in csv_reader:\n            self.data.append(row)\n\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n\n    def _build(self):\n        for example in self.data:\n            \n            input_ = example[0]\n            target = example[1]\n\n            input_ = \"paraphrase: \"+ input_ + ' </s>'\n            target = target + \" </s>\"\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=self.max_len, pad_to_max_length=True, truncation=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=self.max_len, pad_to_max_length=True,truncation=True, return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:42.841905Z","iopub.execute_input":"2022-07-27T05:18:42.842598Z","iopub.status.idle":"2022-07-27T05:18:45.701516Z","shell.execute_reply.started":"2022-07-27T05:18:42.842561Z","shell.execute_reply":"2022-07-27T05:18:45.700434Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Params Directory**","metadata":{}},{"cell_type":"code","source":"if not os.path.exists('t5_paw_global'):\n    os.makedirs('t5_paw_global')\n\nargs_dict.update({'output_dir': 't5_paw_global','num_train_epochs':1,'max_seq_length':256})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:45.703139Z","iopub.execute_input":"2022-07-27T05:18:45.703524Z","iopub.status.idle":"2022-07-27T05:18:45.711248Z","shell.execute_reply.started":"2022-07-27T05:18:45.703488Z","shell.execute_reply":"2022-07-27T05:18:45.710094Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Training Params**","metadata":{}},{"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    filename=\"checkpoint\" + args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=5\n)\n\ntrain_params = dict(\n    #accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    #early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n    amp_backend='apex'\n)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:45.713413Z","iopub.execute_input":"2022-07-27T05:18:45.714363Z","iopub.status.idle":"2022-07-27T05:18:45.726356Z","shell.execute_reply.started":"2022-07-27T05:18:45.714313Z","shell.execute_reply":"2022-07-27T05:18:45.725311Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Model Defined**","metadata":{}},{"cell_type":"code","source":"model = T5FineTuner(args)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:18:45.731439Z","iopub.execute_input":"2022-07-27T05:18:45.731818Z","iopub.status.idle":"2022-07-27T05:19:25.535585Z","shell.execute_reply.started":"2022-07-27T05:18:45.731790Z","shell.execute_reply":"2022-07-27T05:19:25.534593Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training and Saving**","metadata":{}},{"cell_type":"code","source":"import csv\ntrainer = pl.Trainer(**train_params)\n\nprint (\" Training model\")\ntrainer.fit(model)\n\nprint (\"training finished\")\n\nprint (\"Saving model\")\nmodel.model.save_pretrained('t5_paw_paraphrase')\n\nprint (\"Saved model\")","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:19:25.536900Z","iopub.execute_input":"2022-07-27T05:19:25.538991Z","iopub.status.idle":"2022-07-27T05:50:31.991672Z","shell.execute_reply.started":"2022-07-27T05:19:25.538948Z","shell.execute_reply":"2022-07-27T05:50:31.990536Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5_paw_paraphrase')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\nmodel.to(\"cuda\")\n\nsentence = \"The red grapes have been grown in a green aivy in the middle of the street.\"\n\ntext =  \"paraphrase: \" + sentence + \" </s>\"\n\n\nmax_len = 256\n\nencoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\n\nbeam_outputs = model.generate(\n    input_ids=input_ids, attention_mask=attention_masks,\n    do_sample=True,\n    max_length=256,\n    top_k=250,\n    top_p=0.98,\n    early_stopping=True,\n    num_return_sequences=3\n)\n\nprint (\"\\nOriginal Sentence ::\")\nprint (sentence)\nprint (\"\\n\")\nprint (\"Paraphrased Sentence :: \")\nfinal_outputs =[]\nfor beam_output in beam_outputs:\n    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    if sent.lower() != sentence.lower() and sent not in final_outputs:\n        final_outputs.append(sent)\n\nfor i, final_output in enumerate(final_outputs):\n    print(\"{}: {}\".format(i, final_output))","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:50:31.993961Z","iopub.execute_input":"2022-07-27T05:50:31.994361Z","iopub.status.idle":"2022-07-27T05:50:38.354151Z","shell.execute_reply.started":"2022-07-27T05:50:31.994315Z","shell.execute_reply":"2022-07-27T05:50:38.352795Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluate Score with METEOR Metrics**","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/evaluate@a45df1eb9996eec64ec3282ebe554061cb366388\n! pip install datasets\n! pip install nltk\n!pip install -U nltk","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:50:38.356074Z","iopub.execute_input":"2022-07-27T05:50:38.356488Z","iopub.status.idle":"2022-07-27T05:51:27.920067Z","shell.execute_reply.started":"2022-07-27T05:50:38.356448Z","shell.execute_reply":"2022-07-27T05:51:27.918731Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_examples = []\n\ninput = pd.read_csv('../input/paw-paraphrase-generation/PAW_Test.csv')\ninput.head(n = 1000)\ninput=input.to_numpy(dtype=None, copy=False)\n\nfor i in range(input.shape[0]):\n    input_sent = input[i,0]\n    output_sent = input[i,1]\n    test_examples.append([[input_sent],[output_sent] , [] , []]) #third column is the output of the model \n    \nprint (test_examples[7])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:51:27.922570Z","iopub.execute_input":"2022-07-27T05:51:27.922987Z","iopub.status.idle":"2022-07-27T05:51:28.012098Z","shell.execute_reply.started":"2022-07-27T05:51:27.922945Z","shell.execute_reply":"2022-07-27T05:51:28.010953Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import datasets\nimport numpy as np\nfrom datasets.config import importlib_metadata, version\nfrom nltk.translate import meteor_score\n\nimport evaluate\n\n\nNLTK_VERSION = version.parse(importlib_metadata.version(\"nltk\"))\nif NLTK_VERSION >= version.Version(\"3.6.4\"):\n    from nltk import word_tokenize\n\n\n_CITATION = \"\"\"\\\n@inproceedings{banarjee2005,\n  title     = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},\n  author    = {Banerjee, Satanjeev  and Lavie, Alon},\n  booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},\n  month     = jun,\n  year      = {2005},\n  address   = {Ann Arbor, Michigan},\n  publisher = {Association for Computational Linguistics},\n  url       = {https://www.aclweb.org/anthology/W05-0909},\n  pages     = {65--72},\n}\n\"\"\"\n\n_DESCRIPTION = \"\"\"\\\nMETEOR, an automatic metric for machine translation evaluation\nthat is based on a generalized concept of unigram matching between the\nmachine-produced translation and human-produced reference translations.\nUnigrams can be matched based on their surface forms, stemmed forms,\nand meanings; furthermore, METEOR can be easily extended to include more\nadvanced matching strategies. Once all generalized unigram matches\nbetween the two strings have been found, METEOR computes a score for\nthis matching using a combination of unigram-precision, unigram-recall, and\na measure of fragmentation that is designed to directly capture how\nwell-ordered the matched words in the machine translation are in relation\nto the reference.\nMETEOR gets an R correlation value of 0.347 with human evaluation on the Arabic\ndata and 0.331 on the Chinese data. This is shown to be an improvement on\nusing simply unigram-precision, unigram-recall and their harmonic F1\ncombination.\n\"\"\"\n\n_KWARGS_DESCRIPTION = \"\"\"\nComputes METEOR score of translated segments against one or more references.\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    alpha: Parameter for controlling relative weights of precision and recall. default: 0.9\n    beta: Parameter for controlling shape of penalty as a function of fragmentation. default: 3\n    gamma: Relative weight assigned to fragmentation penalty. default: 0.5\nReturns:\n    'meteor': meteor score.\nExamples:\n    >>> meteor = evaluate.load('meteor')\n    >>> predictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\n    >>> references = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\n    >>> results = meteor.compute(predictions=predictions, references=references)\n    >>> print(round(results[\"meteor\"], 4))\n    0.6944\n\"\"\"\n\n\n@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\nclass Meteor(evaluate.Metric):\n    def _info(self):\n        return evaluate.MetricInfo(\n            description=_DESCRIPTION,\n            citation=_CITATION,\n            inputs_description=_KWARGS_DESCRIPTION,\n            features=[\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Sequence(datasets.Value(\"string\", id=\"sequence\"), id=\"references\"),\n                    }\n                ),\n                datasets.Features(\n                    {\n                        \"predictions\": datasets.Value(\"string\", id=\"sequence\"),\n                        \"references\": datasets.Value(\"string\", id=\"sequence\"),\n                    }\n                ),\n            ],\n            codebase_urls=[\"https://github.com/nltk/nltk/blob/develop/nltk/translate/meteor_score.py\"],\n            reference_urls=[\n                \"https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.meteor_score\",\n                \"https://en.wikipedia.org/wiki/METEOR\",\n            ],\n        )\n\n    def _download_and_prepare(self, dl_manager):\n        import nltk\n\n        nltk.download(\"wordnet\")\n        if NLTK_VERSION >= version.Version(\"3.6.5\"):\n            nltk.download(\"punkt\")\n        if NLTK_VERSION >= version.Version(\"3.6.6\"):\n            nltk.download(\"omw-1.4\")\n\n    def _compute(self, predictions, references, alpha=0.9, beta=3, gamma=0.5):\n        multiple_refs = isinstance(references[0], list)\n        if NLTK_VERSION >= version.Version(\"3.6.5\"):\n            # the version of METEOR in NLTK version 3.6.5 and earlier expect tokenized inputs\n            if multiple_refs:\n                scores = [\n                    meteor_score.meteor_score(\n                        [word_tokenize(ref) for ref in refs],\n                        word_tokenize(pred),\n                        alpha=alpha,\n                        beta=beta,\n                        gamma=gamma,\n                    )\n                    for refs, pred in zip(references, predictions)\n                ]\n            else:\n                scores = [\n                    meteor_score.single_meteor_score(\n                        word_tokenize(ref), word_tokenize(pred), alpha=alpha, beta=beta, gamma=gamma\n                    )\n                    for ref, pred in zip(references, predictions)\n                ]\n        else:\n            if multiple_refs:\n                scores = [\n                    meteor_score.meteor_score(\n                        [[word_tokenize(ref) for ref in group] for group in references][0],\n                        word_tokenize(pred),\n                        alpha=alpha,\n                        beta=beta,\n                        gamma=gamma,\n                    )\n                    for ref, pred in zip(references, predictions)\n                ]\n            else:\n                scores = [\n                    meteor_score.single_meteor_score(ref, pred, alpha=alpha, beta=beta, gamma=gamma)\n                    for ref, pred in zip(references, predictions)\n                ]\n\n        return {\"meteor\": np.mean(scores)}","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:51:28.014194Z","iopub.execute_input":"2022-07-27T05:51:28.014965Z","iopub.status.idle":"2022-07-27T05:51:31.874926Z","shell.execute_reply.started":"2022-07-27T05:51:28.014923Z","shell.execute_reply":"2022-07-27T05:51:31.873862Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for i in range (1000):\n    sentence = ''.join(test_examples[i][0])\n\n    text =  \"paraphrase: \" + sentence + \" </s>\"\n\n\n    max_len = 256\n\n    encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n\n    beam_outputs = model.generate(\n        input_ids=input_ids, attention_mask=attention_masks,\n        do_sample=True,\n        max_length=256,\n        top_k=250,\n        top_p=0.98,\n        early_stopping=True,\n        num_return_sequences=3\n    )\n\n\n    for beam_output in beam_outputs:\n        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n        if sent.lower() != sentence.lower():\n            final_outputs= sent\n\n    test_examples [i][2] = final_outputs\n\nprint (test_examples [10][2])","metadata":{"execution":{"iopub.status.busy":"2022-07-27T05:51:31.878707Z","iopub.execute_input":"2022-07-27T05:51:31.879267Z","iopub.status.idle":"2022-07-27T06:03:57.278329Z","shell.execute_reply.started":"2022-07-27T05:51:31.879237Z","shell.execute_reply":"2022-07-27T06:03:57.277118Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"meteor = evaluate.load('meteor')\nvall = 0\nfor i in range (1000):\n    predictions = [test_examples[i][2]]\n    references = (test_examples[i][0]) \n    score = meteor.compute(predictions=predictions, references=references)\n    vall = vall + score.get('meteor')\n\nprint (vall / 1000)","metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:03:57.280304Z","iopub.execute_input":"2022-07-27T06:03:57.280707Z","iopub.status.idle":"2022-07-27T06:04:05.265796Z","shell.execute_reply.started":"2022-07-27T06:03:57.280669Z","shell.execute_reply":"2022-07-27T06:04:05.264660Z"},"trusted":true},"execution_count":17,"outputs":[]}]}